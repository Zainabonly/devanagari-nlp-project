{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fcd0e57-614a-42fa-8cb2-b6f7e721daf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "from json import dump\n",
    "from logging import basicConfig, getLogger\n",
    "from os import linesep, remove\n",
    "from os.path import exists\n",
    "from tempfile import NamedTemporaryFile\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from requests import get\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "from tqdm import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcd98648-0441-4f50-a9d0-e89ee80a8cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "basicConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f268a24d-8d62-47f1-a8c5-53d12bddb3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SentencePieceExtractor:\n",
    "#     \"\"\"\n",
    "#     Extractor implementation for SentencePiece trained models.\n",
    "#     https://github.com/google/sentencepiece\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, model: str):\n",
    "#         # Get SentencePiece\n",
    "#         self.sp = SentencePieceProcessor()\n",
    "#         self.sp.Load(model)\n",
    "\n",
    "#     def extract(self) -> Tuple[Dict[str, int], List[Tuple]]:\n",
    "#         sp = self.sp\n",
    "#         vocab = {sp.id_to_piece(index): index for index in trange(sp.GetPieceSize())}\n",
    "\n",
    "#         # Merges\n",
    "#         merges = []\n",
    "#         for piece_l in tqdm(vocab.keys(), total=sp.GetPieceSize()):\n",
    "#             for piece_r in vocab.keys():\n",
    "#                 merge = f\"{piece_l}{piece_r}\"\n",
    "#                 piece_id = vocab.get(merge, None)\n",
    "#                 if piece_id:\n",
    "#                     merges += [(piece_l, piece_r, piece_id)]\n",
    "#         merges = sorted(merges, key=lambda val: val[2])\n",
    "#         merges = [(val[0], val[1]) for val in merges]\n",
    "\n",
    "#         return vocab, merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "705bdaa8-7306-4969-a76c-bc51bbe61316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import chain\n",
    "# from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# class SentencePieceExtractor:\n",
    "#     \"\"\"\n",
    "#     Extractor implementation for SentencePiece trained models.\n",
    "#     https://github.com/google/sentencepiece\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, model: str):\n",
    "#         # Get SentencePiece\n",
    "#         self.sp = SentencePieceProcessor()\n",
    "#         self.sp.Load(model)\n",
    "    \n",
    "#     def extract(self) -> Tuple[Dict[str, int], List[Tuple]]:\n",
    "#         sp = self.sp\n",
    "#         vocab = {sp.id_to_piece(index): index for index in trange(sp.GetPieceSize())} \n",
    "\n",
    "#         results = []\n",
    "#         with Pool(cpu_count()) as pool:\n",
    "#             results = pool.starmap(self.extract_merges, [(key, vocab,) for key in vocab.keys()])\n",
    "\n",
    "#         # Flatten and filter empty lists\n",
    "#         merges = list(chain.from_iterable(filter(None, results)))\n",
    "\n",
    "#         merges.sort(key=lambda val: val[2])\n",
    "#         merges = [(val[0], val[1]) for val in merges]\n",
    "        \n",
    "#         return vocab, merges\n",
    "\n",
    "#     \"\"\"\n",
    "#     Multiprocessing for merges.\n",
    "#     \"\"\"\n",
    "#     @staticmethod\n",
    "#     def extract_merges(piece_l, vocab):\n",
    "#         merges = []\n",
    "#         for piece_r in vocab.keys():\n",
    "#             merge = f\"{piece_l}{piece_r}\"\n",
    "#             piece_id = vocab.get(merge, None)\n",
    "#             if piece_id:\n",
    "#                 merges += [(piece_l, piece_r, piece_id)]\n",
    "        \n",
    "#         return merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c3d2bdd-cd96-46c1-ae6f-8e93d2cd031c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentencepiece_extractor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      2\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/projects/data/aamod/Libraries/tokenizers/bindings/python/scripts\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentencepiece_extractor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentencePieceExtractor\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sentencepiece_extractor'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/projects/data/aamod/Libraries/tokenizers/bindings/python/scripts\")\n",
    "from sentencepiece_extractor import SentencePieceExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78670f2c-35d2-4f3a-b8bd-91759d48dc1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = SentencePieceProcessor()\n",
    "sp.Load(\"/DATA/rohit/NLP_2025/bpe_mixed.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "416fc7e5-001e-4d9e-b410-e9a65bd75a46",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SentencePieceExtractor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m spe \u001b[38;5;241m=\u001b[39m \u001b[43mSentencePieceExtractor\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/DATA/rohit/NLP_2025/unigram_sanskrit.model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SentencePieceExtractor' is not defined"
     ]
    }
   ],
   "source": [
    "spe = SentencePieceExtractor(\"/DATA/rohit/NLP_2025/unigram_sanskrit.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472efd41-2a3c-4a78-8f49-d9d378952dfb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m v, m \u001b[38;5;241m=\u001b[39m \u001b[43mspe\u001b[49m\u001b[38;5;241m.\u001b[39mextract()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spe' is not defined"
     ]
    }
   ],
   "source": [
    "v, m = spe.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5078c734-5fb2-4bd0-8535-b0c18c72cc9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m v1, m1 \u001b[38;5;241m=\u001b[39m \u001b[43mspe\u001b[49m\u001b[38;5;241m.\u001b[39mextract()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spe' is not defined"
     ]
    }
   ],
   "source": [
    "v1, m1 = spe.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1013d1-cbc2-48b2-951d-64c364663e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e48c18-caa3-4852-b283-52fe962cc850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 == v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176e45fe-8808-4167-90a5-c9569c8a3fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 == m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae1e907-201f-4684-8277-e6c39694f6ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "766d9b41-3dbb-4b63-b7c7-bbc99f9e504b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3200/3200 [00:00<00:00, 393704.29it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = {sp.id_to_piece(index): index for index in trange(sp.GetPieceSize())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95e4587d-65b8-4c9b-9be9-9017059c5767",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "547d62e8-25f4-4481-94c5-740eab4627c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_merges(piece_l, vocab):    \n",
    "    merges = []\n",
    "    for piece_r in vocab.keys():\n",
    "        merge = f\"{piece_l}{piece_r}\"\n",
    "        piece_id = vocab.get(merge, None)\n",
    "        if piece_id:\n",
    "            merges += [(piece_l, piece_r, piece_id)]\n",
    "    \n",
    "    return merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8701788b-89f9-4fc2-8635-63539e26e741",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Think about sorting later, aim should be correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10ed1254-bd39-47f4-829b-f8bc30fb3e16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "with Pool(cpu_count()) as pool:\n",
    "    results = pool.starmap(process_merges, [(k, vocab,) for k in vocab.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8d66d59-620b-4ee7-b87d-c32b890effa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "merges = []\n",
    "for i in results:\n",
    "    if i != []:\n",
    "        merges += i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "775aed80-b51a-42e6-a368-f717343b0d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "merges = sorted(merges, key=lambda val: val[2])\n",
    "merges = [(val[0], val[1]) for val in merges]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a3840cb-e835-4449-9d24-1a39a71948af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99845537-5740-42e4-987d-3ca500f6c2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vocab_fast.json\", \"w\") as f:\n",
    "    json.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c3e56d7-7a5f-408b-90ab-b64630eac4b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70e57cf8-14f1-4902-841b-f93c2c226f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('▁', 'क')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d18d754-182e-4092-842f-a617748401c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"merges_fast.txt\", \"w\") as f:\n",
    "    for item in merges:\n",
    "        f.write(f\"{item[0]} {item[1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6e4a88d-168a-490d-bceb-bf9414825398",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import SentencePieceBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72699861-5a39-4858-8d07-81ef5b7b1937",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SentencePieceBPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ad63440-3e92-45b8-aa2c-2520a89d7938",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tokenizer.from_file(\"vocab_fast.json\", \"merges_fast.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ecd1dd3-6de5-40b6-917e-8a7f7f0ac153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer(vocabulary_size=3200, model=SentencePieceBPE, unk_token=<unk>, replacement=▁, add_prefix_space=True, dropout=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba75b72f-674c-4f22-a1e0-b35973b5c6e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/DATA/rohit/NLP_2025/mehak/vocab.json',\n",
       " '/DATA/rohit/NLP_2025/mehak/merges.txt']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.save_model(\"/DATA/rohit/NLP_2025/mt_bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07630971-b4e2-4a53-b8a8-a152a32e6b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.save(\"/DATA/rohit/NLP_2025/mt_bpe/bpe/tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a9d47b-dbed-4f6b-bb9f-4c2cd7f3d8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"Final/tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df2af4d-c957-433e-8691-e266bcf7de16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Paka_Final/tokenizer_config.json',\n",
       " 'Paka_Final/special_tokens_map.json',\n",
       " 'Paka_Final/tokenizer.json')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"Paka_Final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9381b6bb-59ca-4f85-9683-30f8624e1031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Paka_Final\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b8d953-fa97-440b-9308-3d6a6bdfdde5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13576,\n",
       " 6047,\n",
       " 11980,\n",
       " 5304,\n",
       " 5315,\n",
       " 5522,\n",
       " 124378,\n",
       " 3,\n",
       " 5318,\n",
       " 83741,\n",
       " 5348,\n",
       " 11795,\n",
       " 4,\n",
       " 124378,\n",
       " 124400,\n",
       " 64684]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"How are tyou sff <BOS>ffds dsa<EOS>1!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957fe302-b9ba-4314-ab46-d216323f586e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
